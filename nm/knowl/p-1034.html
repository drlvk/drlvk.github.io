<!DOCTYPE html>
<html lang="en-US">
<!--**************************************-->
<!--*    Generated from PreTeXt source   *-->
<!--*    on 2021-08-19T20:40:44-04:00    *-->
<!--*                                    *-->
<!--*      https://pretextbook.org       *-->
<!--*                                    *-->
<!--**************************************-->
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="robots" content="noindex, nofollow">
</head>
<body>
<h6 class="heading"><span class="type">Paragraph</span></h6>
<p>In <a href="chapter-nonlinear-least-squares.html" class="internal" title="Chapter 29: Nonlinear Least Squares">ChapterÂ 29</a> our training method was to minimize the sum of squares</p>
<div class="displaymath">
\begin{equation*}
S(\mathbf c) = \sum_{j=1}^n (z_j - h_{\mathbf c}(\mathbf x_j))^2
\end{equation*}
</div>
<p data-braille="continuation">However, in the context of probability there is a more natural method: maximizing the <dfn class="terminology">likelihood</dfn> function. Think of \(h_{\mathbf c}(\mathbf x_j)\) as the probability that \(z_j=1\text{,}\) and therefore \(1 - h_{\mathbf c}(\mathbf x_j)\) is the probability that \(z_j=0\text{.}\) Assuming the independence of outcomes, the probability of observing the data that was actually observed is</p>
<div class="displaymath">
\begin{equation}
L(\mathbf c)  = \prod_{z_j = 1}  h_{\mathbf c}(\mathbf x_j)  \cdot \prod_{z_j = 0} (1-h_{\mathbf c}(\mathbf x_j))\tag{36.1.2}
\end{equation}
</div>
<p data-braille="continuation">Since the outcomes \(z_j\) already occurred, this expression is not really the probability of a random event; it is the <dfn class="terminology">likelihood</dfn> of the parameters being \(\mathbf c\text{.}\) We want to maximize this likelihood. Since the product may be very small when there are many observations, it is easier to maximize the <dfn class="terminology">log-likelihood</dfn>, the logarithm of <a class="xref" data-knowl="./knowl/eq-likelihood-function.html" title="Equation 36.1.2">(36.1.2)</a>:</p>
<div class="displaymath">
\begin{equation}
\log L(\mathbf c) = \sum_{z_j = 1} \log h_{\mathbf c}(\mathbf x_j)  + \sum_{z_j = 0} \log (1-h_{\mathbf c}(\mathbf x_j))\tag{36.1.3}
\end{equation}
</div>
<span class="incontext"><a href="section-classification-logistic-regression.html#p-1034">in-context</a></span>
</body>
</html>
