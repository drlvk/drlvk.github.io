<!DOCTYPE html>
<html lang="en-US">
<!--**************************************-->
<!--*    Generated from PreTeXt source   *-->
<!--*    on 2021-04-15T20:06:55-04:00    *-->
<!--*                                    *-->
<!--*      https://pretextbook.org       *-->
<!--*                                    *-->
<!--**************************************-->
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="robots" content="noindex, nofollow">
</head>
<body>
<h6 class="heading"><span class="type">Paragraph</span></h6>
<p>An additional detail: when one reports the model performance (not in this class, but perhaps for a publication), the reported value of \(R^2\) or other estimate of goodness-of-fit should be computed on a separate <dfn class="terminology">validation set</dfn> which was not used for either training or testing. Using the testing set for reporting \(R^2\) introduces a subtle source of bias: if <em class="emphasis">many</em> models are tested, and one of them wins, it is more likely than not that the winning model “had better luck” with the specific data set used for testing, and that would lead us to overestimate its goodness-of-fit. (Here is an <a class="external" href="https://imgs.xkcd.com/comics/significant.png" target="_blank">example</a> of testing too many models and not validating.)</p>
<span class="incontext"><a href="section-overfitting.html#p-793">in-context</a></span>
</body>
</html>
