<!DOCTYPE html>
<html lang="en-US">
<!--**************************************-->
<!--*    Generated from PreTeXt source   *-->
<!--*    on 2020-12-06T21:30:28-05:00    *-->
<!--*                                    *-->
<!--*      https://pretextbook.org       *-->
<!--*                                    *-->
<!--**************************************-->
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="robots" content="noindex, nofollow">
</head>
<body>
<h6 class="heading"><span class="type">Paragraph</span></h6>
<p>In order to minimize a function \(f\colon \mathbb R^n \to \mathbb R\text{,}\) we can start with initial vector \(\mathbf a\) and compute \(\mathbf x = \mathbf a - \beta \nabla f(\mathbf a)\text{,}\) then replace \(\mathbf a\) with \(\mathbf x\) and repeat until convergence is achieved (or the limit on steps is reached). This is the \(n\)-dimensional version of the gradient descent method in <a href="section-gradient-descent-1d.html" class="internal" title="Section 31.2: Gradient descent in one dimension">SectionÂ 31.2</a>. We already saw that the choice of \(\beta\) is difficult even in one dimension. It gets worse in several dimensions.</p>
<span class="incontext"><a href="section-gradient-descent-nd.html#p-879">in-context</a></span>
</body>
</html>
